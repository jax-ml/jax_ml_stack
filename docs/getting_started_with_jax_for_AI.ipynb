{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "538cd052",
   "metadata": {},
   "source": [
    "# Getting started with JAX for AI\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jax-ml/jax-ai-stack/blob/main/docs/getting_started_with_jax_for_AI.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd6ba59",
   "metadata": {
    "id": "lN1DEDeMel9r"
   },
   "source": [
    "In this tutorial, you will learn how to get started using [JAX](https://jax.readthedocs.io) and JAX-based libraries to build and train a simple neural network model. JAX is a Python package for hardware accelerator-oriented array computation and program transformation, and is the engine behind cutting-edge AI research and production models at Google and beyond.\n",
    "\n",
    "The JAX API focuses on [array-based](https://jax.readthedocs.io/en/latest/key-concepts.html#jax-arrays-jax-array) computation, and is at the core of a growing [ecosystem](https://jax.readthedocs.io/en/latest/index.html#ecosystem) of various domain-specific tools. This tutorial introduces a part of that ecosystem with a focus on AI, namely:\n",
    "\n",
    "- [Flax](https://flax.readthedocs.io): A machine learning library designed for building and training scalable neural networks in JAX.\n",
    "- [Optax](https://optax.readthedocs.io): A high-performance function optimization library that comes with built-in optimizers and loss functions. It also allows you to create your own such functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272b5bf8",
   "metadata": {
    "id": "1Y92oUSGeoRz"
   },
   "source": [
    "You should be familiar with numerical computing in Python with [NumPy](http://numpy.org) and fundamental concepts of defining, training, and evaluating machine learning models.\n",
    "\n",
    "Once you've worked through this content, visit the [JAX documentation site](http://jax.readthedocs.io/) for a deeper dive into the high performance numerical computing library, and [Flax](https://flax.readthedocs.io) to learn about neural networks in and for JAX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eba11",
   "metadata": {
    "id": "z7sAr0sderhh"
   },
   "source": [
    "## Example: A simple neural network\n",
    "\n",
    "Let's start with a very simple example of using JAX with [Flax](https://flax.readthedocs.io) to define a model and train it on the hand-written digits dataset with the help of [Optax](https://optax.readthedocs.io) for optimization during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e696a2c",
   "metadata": {
    "id": "pOlnhK-EioSk"
   },
   "source": [
    "### Loading the data\n",
    "\n",
    "JAX can work with a variety of data loaders, including [Grain](https://github.com/google/grain), [TensorFlow Datasets](https://github.com/tensorflow/datasets) and [TorchData](https://github.com/pytorch/data), but for simplicity here you can use the well-known [scikit-learn `digits`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5591ec5",
   "metadata": {
    "id": "hKhPLnNxfOHU",
    "outputId": "ac3508f0-ccc6-409b-c719-99a4b8f94bd6"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "print(f\"{digits.data.shape=}\")\n",
    "print(f\"{digits.target.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f80ddaf",
   "metadata": {
    "id": "lst3E34dgrLc"
   },
   "source": [
    "This dataset consists of `8x8` pixelated images of hand-written digits and their corresponding labels. You can visualize a handful of them this way with [`matplotlib`](https://matplotlib.org/stable/tutorials/index):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bf42d8",
   "metadata": {
    "id": "Y8cMntSdfyyT",
    "outputId": "9343a558-cd8c-473c-c109-aa8015c7ae7e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(10, 10, figsize=(6, 6),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='binary', interpolation='gaussian')\n",
    "    ax.text(0.05, 0.05, str(digits.target[i]), transform=ax.transAxes, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d4134c",
   "metadata": {
    "id": "Z3l45KgtfUUo"
   },
   "source": [
    "Next, split the dataset into a training and testing set, and convert these splits into [`jax.Array`s]https://jax.readthedocs.io/en/latest/key-concepts.html#jax-arrays-jax-array) before you feed them into the model.\n",
    "\n",
    "> **Note:** A [`jax.Array`](https://jax.readthedocs.io/en/latest/_autosummary/jax.Array.html#jax.Array) is similar to [`numpy.ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray). You can learn more about `jax.Array`s in the [Key concepts](https://jax.readthedocs.io/en/latest/key-concepts.html#jax-arrays-jax-array) doc on the [JAX documentation site](https://jax.readthedocs.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db986a59",
   "metadata": {
    "id": "6jrYisoPh6TL"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "splits = train_test_split(digits.images, digits.target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45a60f7",
   "metadata": {
    "id": "oMRcwKd4hqOo",
    "outputId": "0ad36290-397b-431d-eba2-ef114daf5ea6"
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "images_train, images_test, label_train, label_test = map(jnp.asarray, splits)\n",
    "print(f\"{images_train.shape=} {label_train.shape=}\")\n",
    "print(f\"{images_test.shape=}  {label_test.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce956d8",
   "metadata": {
    "id": "JzrixENjifiq"
   },
   "source": [
    "### Defining the Flax model\n",
    "\n",
    "Next, subclass [`flax.nnx.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/module.html#flax.nnx.Module) to create a simple [feed-forward](https://en.wikipedia.org/wiki/Feedforward_neural_network) neural network called `SimpleNN`, which is made up of [`flax.nnx.Linear`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/linear.html#flax.nnx.Linear) layers with *scaled exponential linear unit* (SELU) activation functions (using the built-in [`flax.nnx.selu`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.selu)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f87a3e",
   "metadata": {
    "id": "U77VMQwRjTfH"
   },
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "\n",
    "N_FEATURES = 64  # Number of pixels.\n",
    "N_INTERMEDIATE = 100  # Hidden layer size\n",
    "N_TARGETS = 10  # Number of categories.\n",
    "\n",
    "class SimpleNN(nnx.Module):\n",
    "\n",
    "  def __init__(self, *, rngs: nnx.Rngs):\n",
    "    self.layer1 = nnx.Linear(N_FEATURES, N_INTERMEDIATE, rngs=rngs)\n",
    "    self.layer2 = nnx.Linear(N_INTERMEDIATE, N_INTERMEDIATE, rngs=rngs)\n",
    "    self.layer3 = nnx.Linear(N_INTERMEDIATE, N_TARGETS, rngs=rngs)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    x = x.reshape(-1, 64)  # flatten 8x8 images\n",
    "    x = nnx.selu(self.layer1(x))\n",
    "    x = nnx.selu(self.layer2(x))\n",
    "    x = self.layer3(x)\n",
    "    return x\n",
    "\n",
    "model = SimpleNN(rngs=nnx.Rngs(0))\n",
    "\n",
    "nnx.display(model)  # Interactive display if penzai is installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95da2cc",
   "metadata": {},
   "source": [
    "> **Note:** The [`flax.nnx.Module`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/module.html#flax.nnx.Module) system is covered in more detail in the [Flax basics](https://flax.readthedocs.io/en/latest/nnx_basics.html#the-flax-nnx-module-system) tutorial. Notice the use of the [`flax.nnx.Rngs`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/rnglib.html#flax.nnx.Rngs) type - this is the main convenience API in Flax for managing random state. The `Rngs` object can be used to get new unique JAX [pseudorandom number generator (PRNG)](https://jax.readthedocs.io/en/latest/random-numbers.html) keys based on a root PRNG key passed to the constructor. Learn more about PRNGs in the Flax [Randomness](https://flax.readthedocs.io/en/latest/guides/randomness.html) and the [JAX PRNG](https://jax.readthedocs.io/en/latest/random-numbers.html) tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6eb781",
   "metadata": {
    "id": "FIXmNs5-lrEf"
   },
   "source": [
    "### Training the model\n",
    "\n",
    "With the `SimpleNN` model defined, utilize the [Optax](http://optax.readthedocs.io) library to define the `optimizer` and the loss function (`loss_fun()`), and then create a training step (`train_step()`:\n",
    "\n",
    "- Optax has several [common optimizers available](https://optax.readthedocs.io/en/latest/api/optimizers.html#optimizers), such as the Stochastic Gradient Descent [`optax.sgd`](https://optax.readthedocs.io/en/latest/api/optimizers.html#optax.sgd). \n",
    "- Since you have an output layer with each node corresponding to an integer label, an appropriate loss metric would be [`optax.softmax_cross_entropy_with_integer_labels`](https://optax.readthedocs.io/en/latest/api/losses.html#optax.losses.softmax_cross_entropy_with_integer_labels).\n",
    "- Then, define a `train_step()` based on this optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20624075",
   "metadata": {
    "id": "QwRvFPkYl5b2"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import optax\n",
    "\n",
    "optimizer = nnx.Optimizer(model, optax.sgd(learning_rate=0.05))\n",
    "\n",
    "def loss_fun(\n",
    "    model: nnx.Module,\n",
    "    data: jax.Array,\n",
    "    labels: jax.Array):\n",
    "  logits = model(data)\n",
    "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "    logits=logits, labels=labels\n",
    "  ).mean()\n",
    "  return loss, logits\n",
    "\n",
    "@nnx.jit  # JIT-compile the function.\n",
    "def train_step(\n",
    "    model: nnx.Module,\n",
    "    optimizer: nnx.Optimizer,\n",
    "    data: jax.Array,\n",
    "    labels: jax.Array):\n",
    "  loss_gradient = nnx.value_and_grad(loss_fun, has_aux=True)  # Gradient transform!\n",
    "  (loss, logits), grads = loss_gradient(model, data, labels)\n",
    "  optimizer.update(grads)  # In-place update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4992d67e",
   "metadata": {
    "id": "K2Tp-ym6sXEl"
   },
   "source": [
    "Notice here the use of transformations called [`flax.nnx.jit`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/transforms.html#flax.nnx.jit) and [`flax.nnx.value_and_grad`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/transforms.html#flax.nnx.value_and_grad), which are built on [`jax.jit`](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html) and [`jax.grad`](https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html)/[`jax.value_and_grad`](https://jax.readthedocs.io/en/latest/_autosummary/jax.value_and_grad.html):\n",
    "\n",
    "- `jax.jit` stands for a [just-in-time (JIT) compilation](https://jax.readthedocs.io/en/latest/quickstart.html#just-in-time-compilation-with-jax-jit) transformation. It will cause the function to be passed to the [XLA](https://openxla.org/xla) compiler for fast repeated execution.\n",
    "- `jax.grad` is a gradient transformation. It uses JAX's [automatic differentiation](https://jax.readthedocs.io/en/latest/automatic-differentiation.html) for fast optimization of large networks. `jax.value_and_grad` not only takes the gradient of a function, but also evaluates it.\n",
    "\n",
    "You will return to these transformations further below.\n",
    "\n",
    "> **Note:** For in-depth guides on transformations, check out [Flax transforms](https://flax.readthedocs.io/en/latest/guides/transforms.html) and [JAX](https://jax.readthedocs.io/en/latest/quickstart.html#just-in-time-compilation-with-jax-jit) [transforms](https://jax.readthedocs.io/en/latest/key-concepts.html#transformations).\n",
    "\n",
    "Now, define a training loop to repeatedly perform the `train_step()` over the training data, periodically printing the `loss_fun` against the test set to monitor convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6b7aee",
   "metadata": {
    "id": "l9mukT0eqmsr",
    "outputId": "c6c7b2d6-8706-4bc3-d5a6-0396d7cfbf56"
   },
   "outputs": [],
   "source": [
    "for i in range(301):  # 300 training epochs\n",
    "  train_step(model, optimizer, images_train, label_train)\n",
    "  if i % 50 == 0:  # Print metrics.\n",
    "    loss, _ = loss_fun(model, images_test, label_test)\n",
    "    print(f\"epoch {i}: loss={loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bdb709",
   "metadata": {
    "id": "3sjOKxLDv8SS"
   },
   "source": [
    "After 300 training epochs, the model appears to have converged to a target loss of `0.10`. You can check what this does to the accuracy of the labels for each image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7957398b",
   "metadata": {
    "id": "6OmW0lVlsvJ1",
    "outputId": "f8d7849b-4242-48e7-8120-82e5574b18f3"
   },
   "outputs": [],
   "source": [
    "label_pred = model(images_test).argmax(axis=1)\n",
    "num_matches = jnp.count_nonzero(label_pred == label_test)\n",
    "num_total = len(label_test)\n",
    "accuracy = num_matches / num_total\n",
    "print(f\"{num_matches} labels match out of {num_total}:\"\n",
    "      f\" accuracy = {num_matches/num_total:%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b07112c",
   "metadata": {
    "id": "vTKF3-CFwY50"
   },
   "source": [
    "The simple feed-forward network achieved approximately 98% accuracy on the test set.\n",
    "\n",
    "Create a visualization to check how the model predicted some digits correctly (in green) and incorrectly (in red):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49674b46",
   "metadata": {
    "id": "uinijfm-qXsP",
    "outputId": "632f6e98-1779-4492-c2f7-125499c5b55f"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, figsize=(6, 6),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(images_test[i], cmap='binary', interpolation='gaussian')\n",
    "    color = 'green' if label_pred[i] == label_test[i] else 'red'\n",
    "    ax.text(0.05, 0.05, str(label_pred[i]), transform=ax.transAxes, color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37e18d5",
   "metadata": {
    "id": "x7IIiVymuTRa"
   },
   "source": [
    "In this section, you have learned the basics of using JAX for machine learning with Flax and Optax.\n",
    "\n",
    "> **Note:** To learn more about the Flax fundamentals, go to the [Flax basics](https://flax.readthedocs.io/en/latest/nnx_basics.html) tutorial, which also covers Optax. Flax also includes a number of useful APIs for tracking metrics during training - check them action in the [Flax MNIST tutorial](https://flax.readthedocs.io/en/latest/nnx/mnist_tutorial.html) on the Flax website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69ee71f",
   "metadata": {
    "id": "5ZfGvXAiy2yr"
   },
   "source": [
    "## Key features of JAX\n",
    "\n",
    "The Flax neural network API demonstrated in the first example takes advantage of a number of key [JAX API features](https://jax.readthedocs.io/en/latest/quickstart.html), designed into the library from the ground up. In particular:\n",
    "\n",
    "- [**JAX provides a familiar NumPy-like API for array computing**](https://jax.readthedocs.io/en/latest/quickstart.html#jax-as-numpy).\n",
    "  This means that when processing data and outputs you can reach for APIs like [`jax.numpy.count_nonzero`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.count_nonzero.html), which mirror the familiar APIs of the NumPy package; in this case - [`numpy.count_nonzero`](https://numpy.org/doc/stable/reference/generated/numpy.count_nonzero.html).\n",
    "\n",
    "- [**JAX provides just-in-time (JIT) compilation**](https://jax.readthedocs.io/en/latest/quickstart.html#just-in-time-compilation-with-jax-jit).\n",
    "  This means that you can implement your code easily in Python, but count on fast compiled execution on CPU, GPU, and TPU backends via the [XLA](https://openxla.org/xla) compiler by wrapping your code with a simple [`jax.jit`](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html) transformation.\n",
    "\n",
    "- [**JAX provides automatic differentiation (autodiff).**](https://jax.readthedocs.io/en/latest/quickstart.html#just-in-time-compilation-with-jax-jit)\n",
    "  This means that when fitting models, `optax` and `flax` can compute closed-form gradient functions for fast optimization of models, using the [`jax.grad`](https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html) transformation.\n",
    "\n",
    "- [**JAX provides automatic vectorization.**](https://jax.readthedocs.io/en/latest/quickstart.html#auto-vectorization-with-jax-vmap)\n",
    "  Though you didn't use it directly in the previous example, but under the hood Flax takes advantage of JAX's vectorized map ([`jax.vmap`](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html)) transform to automatically convert the loss and gradient functions to efficient batch-aware functions that are just as fast as hand-written versions. This makes JAX implementations simpler and less error-prone.\n",
    "\n",
    "In the next sections, you’ll learn about `jax.numpy`, and the `jax.jit`, `jax.grad` and `jax.vmap` transforms through additional examples.\n",
    "\n",
    "> **Note:** You can also check out the JAX [Quickstart](https://jax.readthedocs.io/en/latest/quickstart.html) and [Key concepts](https://jax.readthedocs.io/en/latest/key-concepts.html) docs to learn more about the JAX NumPy API and various JAX transforms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdf3d86",
   "metadata": {
    "id": "ZjneGfjy2Ef1"
   },
   "source": [
    "### JAX NumPy interface\n",
    "\n",
    "The foundational array computing package in Python is NumPy, and JAX provides a matching API via the [`jax.numpy`](https://jax.readthedocs.io/en/latest/jax.numpy.html#module-jax.numpy) subpackage.\n",
    "\n",
    "Additionally, JAX arrays behave much like NumPy arrays in their attributes, and in terms of [indexing](https://numpy.org/doc/stable/user/basics.indexing.html) and [broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html) semantics.\n",
    "\n",
    "When setting up the `SimpleNN` model in the first example, you used the built-in [`flax.nnx.selu`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/activations.html#flax.nnx.selu) activation function. You can also implement your own version of SELU using the [`jax.numpy`](https://jax.readthedocs.io/en/latest/jax.numpy.html#module-jax.numpy) API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfe11f1",
   "metadata": {
    "id": "2u2femxe2EzA",
    "outputId": "89b9f9b0-5631-405c-f4d8-2198593d0d50"
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "def selu(x, alpha=1.67, lam=1.05):\n",
    "  return lam * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "x = jnp.arange(5.0)\n",
    "print(selu(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d55bba1",
   "metadata": {
    "id": "H9o_a859JLY9"
   },
   "source": [
    "> **Caution:** Despite the broad similarities, be aware that JAX does have some well-motivated differences from NumPy that you can read about in [JAX – the Sharp Bits](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c34b1a",
   "metadata": {
    "id": "LnDgHRBsJrYL"
   },
   "source": [
    "### Just-in-time compilation\n",
    "\n",
    "JAX is built on the [XLA](https://openxla.org/xla) compiler, and allows sequences of operations to be JIT-compiled using the [`jax.jit`](https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html) transformation.\n",
    "\n",
    "In the example above, you used the similar [`flax.nnx.jit`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/transforms.html#flax.nnx.jit) API, which has some special handling for Flax objects to speed up neural network training.\n",
    "\n",
    "Returning to the custom `selu()` function, you can create a JIT-compiled version this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8226df3a",
   "metadata": {
    "id": "-Chp8yCjQaFY"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "selu_jit = jax.jit(selu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122a2d03",
   "metadata": {
    "id": "zAKNrQbxQgC7"
   },
   "source": [
    "`selu_jit` is now a compiled version of the original function, which returns the same result to typical floating-point precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f4420b",
   "metadata": {
    "id": "uHeJXgKURL6q",
    "outputId": "dfc5a602-2b28-4863-a852-38f8fe6aaab4"
   },
   "outputs": [],
   "source": [
    "x = jnp.arange(1E6)\n",
    "jnp.allclose(selu(x), selu_jit(x))  # results match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729445e1",
   "metadata": {
    "id": "WWwD0NmzRLP8"
   },
   "source": [
    "You can use IPython's `%timeit` magic to see the speedup (note the use of `.block_until_ready()`, which you use to account for JAX's [asynchronous dispatch](https://jax.readthedocs.io/en/latest/async_dispatch.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed0882",
   "metadata": {
    "id": "SzU_0NU5Jq_W",
    "outputId": "dba1ee6b-32f8-4429-a147-b6d4f4e6f0ff"
   },
   "outputs": [],
   "source": [
    "%timeit selu(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93831824",
   "metadata": {
    "id": "QOu7wo7UQ07v",
    "outputId": "bd91aaa2-d367-47e0-eb17-a90658de2d14"
   },
   "outputs": [],
   "source": [
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a819d4",
   "metadata": {
    "id": "1ST-uLL9JqzB"
   },
   "source": [
    "For this computation, running on CPU, JIT compilation gives an order of magnitude speedup.\n",
    "JAX's documentation has more discussion of JIT compilation in [this in-depth tutorial](https://jax.readthedocs.io/en/latest/jit-compilation.html).\n",
    "\n",
    "> **Note:** Learn more about `jax.jit` in the JAX [Quickstart](https://jax.readthedocs.io/en/latest/quickstart.html#just-in-time-compilation-with-jax-jit) and the [Just-in-time compilation tutorial](https://jax.readthedocs.io/en/latest/jit-compilation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a44a51d",
   "metadata": {
    "id": "XFWR0tYjLYcj"
   },
   "source": [
    "### Automatic differentiation (`jax.grad`)\n",
    "\n",
    "For efficient optimization of neural network models, fast gradient computations are essential. JAX enables this via its [automatic differentiation](https://jax.readthedocs.io/en/latest/automatic-differentiation.html) transformations like [`jax.grad`](https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html), which computes a closed-form gradient of a JAX function. In the example above, you used the similar [`flax.nnx.grad`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/transforms.html#flax.nnx.grad) function, which has special handling for `flax.nnx` objects.\n",
    "\n",
    "Here's how to compute the gradient of a function with [`jax.grad`](https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1efaa17",
   "metadata": {
    "id": "JtsPYnKbOtZt",
    "outputId": "834c31f8-ed1f-46ae-a827-e0b7faa52181"
   },
   "outputs": [],
   "source": [
    "x = jnp.float32(-1.0)\n",
    "jax.grad(selu)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d917c3",
   "metadata": {
    "id": "1P-UEh9VO94k"
   },
   "source": [
    "You can briefly check with a finite-difference approximation that this is giving the expected value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084a2d44",
   "metadata": {
    "id": "1gOc4FyzPDUC",
    "outputId": "95053e89-048d-4331-b898-079818e23dae"
   },
   "outputs": [],
   "source": [
    "eps = 1E-3\n",
    "(selu(x + eps) - selu(x)) / eps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc0c922",
   "metadata": {
    "id": "pkQW2Hd_bPSd"
   },
   "source": [
    "Importantly, the automatic differentiation approach is both more accurate and efficient than computing numerical gradients. JAX's documentation has more discussion of autodiff at [Automatic differentiation](https://jax.readthedocs.io/en/latest/automatic-differentiation.html).\n",
    "\n",
    "> **Note:** Learn more about `jax.grad` in the JAX [Quickstart](https://jax.readthedocs.io/en/latest/quickstart.html#taking-derivatives-with-jax-grad), as well as the [Automatic differentiation (autodiff)](https://jax.readthedocs.io/en/latest/automatic-differentiation.html) and [Advanced autodiff](https://jax.readthedocs.io/en/latest/advanced-autodiff.html) tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17242cf1",
   "metadata": {
    "id": "xsKyfRDNbj2y"
   },
   "source": [
    "### Automatic vectorization (`jax.vmap`)\n",
    "\n",
    "In the training loop above, you defined the loss function in terms of a single input data vector of shape `n_features`, but trained the model by passing batches of data (of shape `[n_samples, n_features]`). Rather than requiring a naive and slow loop over batches in Flax and Optax internals, they instead use JAX's automatic vectorization via the `jax.vmap` transformation to construct a batched version of the kernel automatically.\n",
    "\n",
    "Consider a simple loss function that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e18d964",
   "metadata": {
    "id": "OuSSCpxzdWw_"
   },
   "outputs": [],
   "source": [
    "def loss(x: jax.Array, x0: jax.Array):\n",
    "  return jnp.sum((x - x0) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af31aa97",
   "metadata": {
    "id": "lOg9IWlPddfE"
   },
   "source": [
    "You can evaluate it on a single data vector this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b61e1b",
   "metadata": {
    "id": "sYlEtbxedngb",
    "outputId": "39030fb7-feee-4da1-ef5d-54cd86ad8dfb"
   },
   "outputs": [],
   "source": [
    "x = jnp.arange(3.)\n",
    "x0 = jnp.ones(3)\n",
    "loss(x, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef097a7",
   "metadata": {
    "id": "STit-syzk59F"
   },
   "source": [
    "But if you attempt to evaluate it on a batch of vectors, it does not correctly return a batch of 4 losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0ce15f",
   "metadata": {
    "id": "1LFQX3zGlCil",
    "outputId": "a12c4d75-2d94-4341-e9ca-915a33f1278e"
   },
   "outputs": [],
   "source": [
    "batched_x = jnp.arange(12).reshape(4, 3)  # batch of 4 vectors\n",
    "loss(batched_x, x0)  # wrong!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e808588",
   "metadata": {
    "id": "Qc3Kwe2HlhpA"
   },
   "source": [
    "The problem is that the loss function is not batch-aware. Without automatic vectorization, there are two ways you can address this:\n",
    "\n",
    "1. Re-write your loss function by hand to operate on batched data. However, as functions become more complicated, this becomes difficult and error-prone.\n",
    "2. Naively loop over unbatched calls to your original function. This is easy to code, but can be slow because it doesn't take advantage of vectorized compute.\n",
    "\n",
    "The `jax.vmap` transformation offers a third way: it automatically transforms your original function into a batch-aware version, so you get the speed of option 1 with the ease of option 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bdd743",
   "metadata": {
    "id": "Y2Sa458OoRVL",
    "outputId": "d1d8295b-40d3-477a-e5d8-b2d6f28ad803"
   },
   "outputs": [],
   "source": [
    "loss_batched = jax.vmap(loss, in_axes=(0, None))  # batch x over axis 0, do not batch x0\n",
    "loss_batched(batched_x, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEQPh3NtawWA"
   },
   "source": [
    "In the neural network example you learned to build and train, both Flax and Optax make use of JAX's `jax.vmap` to allow for efficient batched computations over the unbatched loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lN1DEDeMel9r"
   },
   "source": [
    "> **Note:** Learn more about `jax.vmap` in the JAX [Quickstart](https://jax.readthedocs.io/en/latest/quickstart.html#auto-vectorization-with-jax-vmap) and the [Automatic vectorization](https://jax.readthedocs.io/en/latest/automatic-vectorization.html) tutorial."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md:myst",
   "main_language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
